{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "import geocoder\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "import pandas as pd\n",
    "import pydeck as pdk\n",
    "from branca.colormap import LinearColormap\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from srai.regionalizers import VoronoiRegionalizer, geocode_to_region_gdf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://wybory.gov.pl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polling_districts_with_urls(tr) -> tuple[int, str, str]:\n",
    "    a_href = tr.select_one(\"a\", href=True)\n",
    "    a_href.find(\"div\").decompose()\n",
    "\n",
    "    address = tr.find_all(\"td\")[-1].text\n",
    "\n",
    "    return (\n",
    "        int(a_href.text.strip().replace(\" \", \"\").replace(\"\\xa0\", \"\")),\n",
    "        a_href[\"href\"],\n",
    "        address,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_votes_from_polling_district(driver, url) -> tuple[dict, int, int]:\n",
    "    # president\n",
    "    driver.get(url)\n",
    "    delay = 5  # seconds\n",
    "\n",
    "    WebDriverWait(driver, delay).until(\n",
    "        EC.presence_of_element_located((By.ID, \"obkw_can_cont_4_1\"))\n",
    "    )\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    turnout_div = soup.select_one(\"div.turnout\").select_one(\"table\")\n",
    "    turnout_tbl = pd.read_html(StringIO(str(turnout_div)))[0]\n",
    "    voters_possible = int(\n",
    "        turnout_tbl[\n",
    "            turnout_tbl[0].str.contains(\"Liczba wyborców\")\n",
    "            & turnout_tbl[0].str.contains(\"Prezydenta\")\n",
    "        ]\n",
    "        .iloc[0][1]\n",
    "        .replace(\"\\xa0\", \"\")\n",
    "    )\n",
    "\n",
    "    voters_voted = int(\n",
    "        turnout_tbl[\n",
    "            turnout_tbl[0].str.contains(\"Liczba kart\")\n",
    "            & turnout_tbl[0].str.contains(\"Prezydenta\")\n",
    "        ]\n",
    "        .iloc[0][1]\n",
    "        .replace(\"\\xa0\", \"\")\n",
    "    )\n",
    "\n",
    "    candidates_div = soup.select_one(\"div#obkw_can_cont_4_1\").select_one(\"table\")\n",
    "    candidates_tbl = pd.read_html(StringIO(str(candidates_div).replace(\"\\xa0\", \"\")))[0]\n",
    "    votes_per_candidate = (\n",
    "        candidates_tbl[[\"Nazwisko i imiona\", \"Liczba głosów na kandydata\"]]\n",
    "        .set_index(\"Nazwisko i imiona\")[\"Liczba głosów na kandydata\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "    return voters_possible, voters_voted, votes_per_candidate\n",
    "\n",
    "\n",
    "def download_data_for_city(city_code: str):\n",
    "    if (\n",
    "        not Path(f\"../../output_data/{city_code}_polling_districts_data.json\").exists()\n",
    "        or not Path(f\"../../output_data/{city_code}_h3_votes_data.json\").exists()\n",
    "        or not Path(f\"../../output_data/{city_code}_geo_data.geojson\").exists()\n",
    "        or not Path(f\"../../output_data/{city_code}_voronoi_data.geojson\").exists()\n",
    "    ):\n",
    "        city_data = (\n",
    "            pd.read_csv(\"../../input_data/nec_urls.csv\", sep=\",\")\n",
    "            .query(f'city == \"{city_code}\"')\n",
    "            .iloc[0]\n",
    "        )\n",
    "        \n",
    "        geocoded_addresses = {}\n",
    "        polling_districts = {}\n",
    "\n",
    "        driver = webdriver.Firefox()\n",
    "\n",
    "        if not Path(f\"../input_data/{city_code}.csv\").exists():\n",
    "            scraping_url = city_data.url\n",
    "\n",
    "            driver.get(scraping_url)\n",
    "            delay = 5  # seconds\n",
    "\n",
    "            WebDriverWait(driver, delay).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"obkw\"))\n",
    "            )\n",
    "\n",
    "            div = None\n",
    "            while not div:\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                div = soup.select_one(\"div.obkw\").select_one(\"table\")\n",
    "\n",
    "            for tr in tqdm(div.find_all(\"tr\")[1:]):\n",
    "                number, district_url, address = get_polling_districts_with_urls(tr)\n",
    "                if address not in geocoded_addresses:\n",
    "                    geocoded_addresses[address] = geocoder.arcgis(address).json\n",
    "\n",
    "                if not geocoded_addresses[address][\"ok\"]:\n",
    "                    print(\"Can't geocode:\", geocoded_addresses[address])\n",
    "                    continue\n",
    "\n",
    "                polling_districts[number] = (district_url, geocoded_addresses[address])\n",
    "\n",
    "            polling_districts_df = pd.DataFrame(\n",
    "                [\n",
    "                    dict(idx=k, url=v[0], lon=v[1][\"lng\"], lat=v[1][\"lat\"])\n",
    "                    for k, v in polling_districts.items()\n",
    "                ]\n",
    "            )\n",
    "            polling_districts_df.to_csv(f\"../../input_data/{city_code}.csv\", index=False)\n",
    "\n",
    "        polling_districts_df = pd.read_csv(f\"../../input_data/{city_code}.csv\")\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            data=polling_districts_df,\n",
    "            geometry=gpd.GeoSeries.from_xy(\n",
    "                polling_districts_df.lon, polling_districts_df.lat, crs=4326\n",
    "            ),\n",
    "        )\n",
    "        gdf[\"h3\"] = gdf.geometry.apply(lambda pt: h3.latlng_to_cell(pt.x, pt.y, 15))\n",
    "\n",
    "        votes_per_district = {}\n",
    "        votes_possible_per_district = {}\n",
    "        votes_voted_per_district = {}\n",
    "        for _, row in tqdm(gdf.iterrows(), total=len(gdf)):\n",
    "            idx = row.idx\n",
    "            url = base_url + row.url\n",
    "            voters_possible, voters_voted, votes_per_candidate = (\n",
    "                get_votes_from_polling_district(driver, url)\n",
    "            )\n",
    "            votes_per_district[idx] = votes_per_candidate\n",
    "            votes_possible_per_district[idx] = voters_possible\n",
    "            votes_voted_per_district[idx] = voters_voted\n",
    "\n",
    "        rows = []\n",
    "        for k, v in votes_per_district.items():\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"idx\": k,\n",
    "                    **v,\n",
    "                    \"voters_total\": votes_possible_per_district[k],\n",
    "                    \"voters_voted\": votes_voted_per_district[k],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        df_votes_per_district = pd.DataFrame(rows)\n",
    "        joined_full_data = gdf.merge(df_votes_per_district, on=\"idx\")\n",
    "        joined_full_data = joined_full_data[joined_full_data[\"voters_voted\"] >= 50]\n",
    "\n",
    "        candidates_names = list(\n",
    "            votes_per_district[list(votes_per_district.keys())[0]].keys()\n",
    "        )\n",
    "        for candidate_name in candidates_names:\n",
    "            joined_full_data[candidate_name] = (\n",
    "                joined_full_data[candidate_name]\n",
    "                .apply(lambda x: x.replace(\"\\xa0\", \"\") if isinstance(x, str) else x)\n",
    "                .astype(int)\n",
    "            )\n",
    "\n",
    "        polling_districts_data = joined_full_data[\n",
    "            [\n",
    "                \"h3\",\n",
    "                \"idx\",\n",
    "                *candidates_names,\n",
    "                \"voters_total\",\n",
    "                \"voters_voted\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        polling_districts_data[\"total_votes\"] = polling_districts_data[\n",
    "            candidates_names\n",
    "        ].sum(axis=1)\n",
    "        for candidate_name in candidates_names:\n",
    "            polling_districts_data[f\"{candidate_name}_%\"] = (\n",
    "                polling_districts_data[candidate_name]\n",
    "                / polling_districts_data[\"total_votes\"]\n",
    "            )\n",
    "\n",
    "        polling_districts_data[\"turnout_%\"] = polling_districts_data[\n",
    "            \"voters_voted\"\n",
    "        ].astype(float) / polling_districts_data[\"voters_total\"].astype(float)\n",
    "\n",
    "        polling_districts_data.to_json(\n",
    "            f\"../../output_data/{city_code}_polling_districts_data.json\", orient=\"records\"\n",
    "        )\n",
    "\n",
    "        h3_votes_data = (\n",
    "            joined_full_data[[\"h3\", *candidates_names, \"voters_voted\", \"voters_total\"]]\n",
    "            .groupby(\"h3\")\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        h3_votes_data[\"total_votes\"] = h3_votes_data[candidates_names].sum(axis=1)\n",
    "        for candidate_name in candidates_names:\n",
    "            h3_votes_data[f\"{candidate_name}_%\"] = (\n",
    "                h3_votes_data[candidate_name] / h3_votes_data[\"total_votes\"]\n",
    "            )\n",
    "\n",
    "        h3_votes_data[\"turnout_%\"] = (\n",
    "            h3_votes_data[\"voters_voted\"] / h3_votes_data[\"voters_total\"]\n",
    "        )\n",
    "\n",
    "        h3_votes_data.to_json(\n",
    "            f\"../../output_data/{city_code}_h3_votes_data.json\", orient=\"records\"\n",
    "        )\n",
    "\n",
    "        geo_distinct_data = (\n",
    "            joined_full_data[\n",
    "                [\n",
    "                    \"h3\",\n",
    "                    \"geometry\",\n",
    "                ]\n",
    "            ]\n",
    "            .groupby(\"h3\")\n",
    "            .first()\n",
    "        )\n",
    "        geo_distinct_data = gpd.GeoDataFrame(geo_distinct_data).set_crs(\n",
    "            4326, allow_override=True\n",
    "        )\n",
    "\n",
    "        geo_distinct_data.to_file(f\"../../output_data/{city_code}_geo_data.geojson\")\n",
    "\n",
    "        area = geocode_to_region_gdf(city_data.geocode)\n",
    "\n",
    "        voronoi_regions = VoronoiRegionalizer(seeds=geo_distinct_data).transform(area)\n",
    "        voronoi_regions.to_file(f\"../../output_data/{city_code}_voronoi_data.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\n",
    "    'gda', 'poz', 'kra', ''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_city(CITY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
